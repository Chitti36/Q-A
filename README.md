# ğŸ§  QnA Assistant with PDF + Chat

This project is a **Streamlit-based Chat Assistant** that can:
- Answer general questions using OpenAI GPT models
- Extract and answer questions from uploaded PDF documents using **Groq LLM** + **RAG**
- Maintain session-aware chat history for context-rich interactions

## ğŸš€ Features

- ğŸ“„ Upload PDFs and query them using LLMs (RAG with LangChain)
- ğŸ¤– Choose from multiple OpenAI models: `gpt-4`, `gpt-4-turbo`, `gpt-4o`
- âš™ï¸ Powered by:
  - **LangChain**
  - **HuggingFace Embeddings**
  - **ChromaDB Vector Store**
  - **Groq + OpenAI**
- ğŸ§  Maintains chat memory per session

## ğŸ“‚ Folder Contents

```
project/
â”œâ”€â”€ app.py              # Main Streamlit app
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .env                # API keys (not committed)
```

## ğŸ§ª Requirements

Install all dependencies using:

```bash
pip install -r requirements.txt
```

Make sure you have Python 3.10+.

## ğŸ” Environment Setup

Create a `.env` file in the root with the following:

```env
OPENAI_API_KEY=your-openai-api-key
GROQ_API_KEY=your-groq-api-key
HF_TOKEN=your-huggingface-token
```

> `HF_TOKEN` is used for HuggingFace Embeddings  
> `GROQ_API_KEY` is required to use Groq's `llama3` model on PDFs  
> `OPENAI_API_KEY` is required for general chat using OpenAI

## â–¶ï¸ Running the App

```bash
streamlit run app.py
```

## ğŸ’¡ Behavior

- **PDF Uploaded** â†’ Uses Groq + RAG to answer PDF-related questions
- **No PDF** â†’ Defaults to OpenAI GPT chat
- **Chat Memory**:
  - Session-aware memory via LangChain for Groq
  - App-level history for OpenAI chat

## ğŸ› ï¸ Tech Stack

- `Streamlit` for the UI
- `LangChain` to manage chaining and memory
- `ChromaDB` as vectorstore
- `HuggingFace` Sentence Transformers for embedding
- `Groq` + `OpenAI` for LLM inference


## ğŸ‘¨â€ğŸ’» Author

Developed by **Ruthik Chitti**
